{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HEMA1188/NLTK/blob/main/Tokenize_day_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d021ce3",
      "metadata": {
        "id": "4d021ce3"
      },
      "source": [
        "# Word token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36b20bd8",
      "metadata": {
        "id": "36b20bd8",
        "outputId": "61f24181-3f61-49f6-bc9b-3534994d1e32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
          ]
        }
      ],
      "source": [
        "text = \"The quick brown fox jumped over the lazy dog\"\n",
        "\n",
        "tokens = text.split()\n",
        "\n",
        "print(tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f0e8e1",
      "metadata": {
        "id": "f6f0e8e1",
        "outputId": "26e0fc6f-c7ca-465d-d751-6951cd1a414b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['The', 'quick', 'brown', 'fox,', 'with', 'comma']\n"
          ]
        }
      ],
      "source": [
        "text = \"The quick brown fox, with comma\"\n",
        "\n",
        "tokens = text.split()\n",
        "\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c63435f4",
      "metadata": {
        "id": "c63435f4",
        "outputId": "3cec4fb5-94e8-40ce-b973-e91b9617de09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['The', 'quick', 'brown', 'fox', ',', 'with', 'comma']\n"
          ]
        }
      ],
      "source": [
        "#import nltk\n",
        "#nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"The quick brown fox, with comma\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c875d6bd",
      "metadata": {
        "id": "c875d6bd",
        "outputId": "ca1d458b-3dfc-46ab-fe96-db514d6dbe07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\AJANTHA\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import download\n",
        "download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ba9db13",
      "metadata": {
        "id": "2ba9db13"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3039818d",
      "metadata": {
        "id": "3039818d",
        "outputId": "e9376c71-3034-4462-e62d-7c993b3585c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c699bd8b",
      "metadata": {
        "id": "c699bd8b"
      },
      "outputs": [],
      "source": [
        "senten = \"I am learning Python. Make a challenge on popular programming language\"\n",
        "sent_words = word_tokenize(senten)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6532f7e9",
      "metadata": {
        "id": "6532f7e9",
        "outputId": "ff81b81f-b3f1-4642-fb14-56348c7b0f35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'am', 'learning', 'Python', '.', 'Make', 'a', 'challenge', 'on', 'popular', 'programming', 'language']\n"
          ]
        }
      ],
      "source": [
        "print(sent_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51eadcf5",
      "metadata": {
        "id": "51eadcf5"
      },
      "outputs": [],
      "source": [
        "senten = \"I visited the UK from the US on 22-10-22\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0edb3dce",
      "metadata": {
        "id": "0edb3dce"
      },
      "outputs": [],
      "source": [
        "def normalize(text):\n",
        "    return text.replace(\"US\", \"United States\"). replace(\"UK\", \"United Kingdom\").replace(\"-22\", \"-2022\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e37c6f79",
      "metadata": {
        "id": "e37c6f79",
        "outputId": "27e0e7ec-fe2b-4d88-a6d2-a18d11ea2bb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I visited the United Kingdom from the United States on 22-10-2022\n"
          ]
        }
      ],
      "source": [
        "norm_sent = normalize(senten)\n",
        "print(norm_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "092e3eb5",
      "metadata": {
        "id": "092e3eb5",
        "outputId": "2a1d6060-ea3c-40ec-f584-9803196be97a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The United States and United Kingdom are the two superpowers\n"
          ]
        }
      ],
      "source": [
        "norm_sent = normalize(\"The US and UK are the two superpowers\")\n",
        "print(norm_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b297bf25",
      "metadata": {
        "id": "b297bf25",
        "outputId": "9967b813-f041-49d7-a0d5-2f97c941f652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: autocorrect in c:\\users\\ajantha\\anaconda3\\lib\\site-packages (2.6.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install autocorrect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "652b76e0",
      "metadata": {
        "id": "652b76e0"
      },
      "outputs": [],
      "source": [
        "from autocorrect import Speller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07f8d00a",
      "metadata": {
        "id": "07f8d00a",
        "outputId": "955b8e4b-265a-437e-b611-0b5a55125d6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Natural'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spell = Speller(lang = 'en')\n",
        "spell('Natureal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd3a5edd",
      "metadata": {
        "id": "dd3a5edd",
        "outputId": "f39eb36b-7dc4-41e3-a7bf-942bea10854f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ntural', 'Luanguage', 'Processin', 'deals', 'with', 'art', 'of', 'extracting', 'insightes', 'from', 'Natureal', 'Languaagge']\n"
          ]
        }
      ],
      "source": [
        "sente = word_tokenize(\"Ntural Luanguage Processin deals with art of extracting insightes from Natureal Languaagge\")\n",
        "print(sente)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18f25d9a",
      "metadata": {
        "id": "18f25d9a"
      },
      "outputs": [],
      "source": [
        "def correct_spell(tokens):\n",
        "    senten_corrected = ' '.join([spell(word) for word in tokens])\n",
        "    return senten_corrected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823039e3",
      "metadata": {
        "id": "823039e3",
        "outputId": "2c92174b-3002-4651-f151-de9294436f52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural Language Processing deals with art of extracting insights from Natural Language\n"
          ]
        }
      ],
      "source": [
        "print(correct_spell(sente))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66f22ade",
      "metadata": {
        "id": "66f22ade",
        "outputId": "b0934da4-042e-4e78-b525-813b1e408b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Tweet', 'about', '#', 'NLP', '@', 'Guvi', ':', ')']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Tweet about #NLP @Guvi :)\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8481cd9",
      "metadata": {
        "id": "e8481cd9",
        "outputId": "43b82fb8-4f6d-466c-85fb-a3eb31696aa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Tweet', 'about', '#NLP', '@Guvi', ':)']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "text = \"Tweet about #NLP @Guvi :)\"\n",
        "tokenizer = TweetTokenizer()\n",
        "tokens =tokenizer.tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d04108",
      "metadata": {
        "id": "b0d04108",
        "outputId": "2b9e60c4-0077-4e16-c669-b36ba6d38d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Tweet', 'about', '#NLP', ':)']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "text = \"Tweet about #NLP @Guvi :)\"\n",
        "tokenizer = TweetTokenizer(strip_handles = True)\n",
        "tokens =tokenizer.tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4755d19f",
      "metadata": {
        "id": "4755d19f",
        "outputId": "86d079de-e29d-412f-fd58-704e1eabbf8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['How', 'about', 'currencies', 'like', '$', '250.00', 'and', 'dates', 'like', '19th', 'December']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"How about currencies like $250.00 and dates like 19th December\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46d98fa8",
      "metadata": {
        "id": "46d98fa8"
      },
      "source": [
        "# Normalize\n",
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9c4b52b",
      "metadata": {
        "id": "d9c4b52b",
        "outputId": "6615fb5c-c4b6-4c42-ec47-97ba21de8f88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "have\n",
            "have\n",
            "had\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "s = PorterStemmer()\n",
        "\n",
        "print(s.stem(\"Having\"))\n",
        "print(s.stem(\"Have\"))\n",
        "print(s.stem(\"Had\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3716acab",
      "metadata": {
        "id": "3716acab",
        "outputId": "f674616a-41e6-42c8-e799-fcc195a479b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fish\n",
            "fish\n",
            "fisher\n",
            "fish\n",
            "fish\n"
          ]
        }
      ],
      "source": [
        "print(s.stem(\"Fishing\"))\n",
        "print(s.stem(\"Fish\"))\n",
        "print(s.stem(\"Fisher\"))\n",
        "print(s.stem(\"Fishes\"))\n",
        "print(s.stem(\"Fished\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a80a24b4",
      "metadata": {
        "id": "a80a24b4"
      },
      "source": [
        "# Lemmatisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84faf906",
      "metadata": {
        "id": "84faf906",
        "outputId": "1f199cfe-6180-417f-c941-e2128b079cff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\AJANTHA\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "s = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d26f79ab",
      "metadata": {
        "id": "d26f79ab",
        "outputId": "74ad6206-605c-43f8-9a4b-e5a88da060d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "have\n",
            "have\n",
            "have\n"
          ]
        }
      ],
      "source": [
        "print(s.lemmatize('having',pos = 'v'))\n",
        "print(s.lemmatize('have',pos = 'v'))\n",
        "print(s.lemmatize('had',pos = 'v'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4aed1e6",
      "metadata": {
        "id": "e4aed1e6",
        "outputId": "5fc63e41-ac87-41a8-d666-7ed12fe9aef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fishing\n",
            "Fish\n",
            "Fisher\n",
            "Fishes\n",
            "Fished\n"
          ]
        }
      ],
      "source": [
        "print(s.lemmatize('Fishing',pos = 'v'))\n",
        "print(s.lemmatize('Fish',pos = 'v'))\n",
        "print(s.lemmatize('Fisher',pos = 'n'))\n",
        "print(s.lemmatize('Fishes',pos = 'v'))\n",
        "print(s.lemmatize('Fished',pos = 'v'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4972b6cc",
      "metadata": {
        "id": "4972b6cc",
        "outputId": "c6011013-1ea6-498c-a1f8-8d0029cfb449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "be\n",
            "be\n",
            "be\n"
          ]
        }
      ],
      "source": [
        "print(s.lemmatize('am',pos = 'v'))\n",
        "print(s.lemmatize('is',pos = 'v'))\n",
        "print(s.lemmatize('was',pos = 'v'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d996b8c",
      "metadata": {
        "id": "8d996b8c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}